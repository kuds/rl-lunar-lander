{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-lunar-lander/blob/main/%5BLunar%20Lander%5D%20Proximal%20Policy%20Optimization%20(PPO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Il1XSnjgFIK"
      },
      "source": [
        "# Proximal Policy Optimization (PPO)\n",
        "---\n",
        "In this notebook, you will implement a PPO agent with OpenAI Gym's LunarLander-v2 environment.\n",
        "\n",
        "### 1. Import the Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "RIaNzWP4iXFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3 gymnasium gymnasium[box2d]"
      ],
      "metadata": {
        "id": "eZaOFVOwhl1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bhW4cVvKgFIL"
      },
      "outputs": [],
      "source": [
        "import gymnasium\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "import scipy\n",
        "import platform\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Python Version: {}\".format(platform.python_version()))\n",
        "print(\"Is Cuda Available: {}\".format(torch.cuda.is_available()))\n",
        "print(\"Torch Version: {}\".format(torch.__version__))\n",
        "print(\"Cuda Version: {}\".format(torch.version.cuda))\n",
        "print(\"Scipy Version: {}\".format(scipy.__version__))\n",
        "print(\"Numpy Version: {}\".format(numpy.__version__))\n",
        "print(\"Stable Baseline Version: {}\".format(stable_baselines3.__version__))\n",
        "print(\"IPython Version: {}\".format(IPython.__version__))\n",
        "print(\"Gymnasium Version: {}\".format(gymnasium.__version__))"
      ],
      "metadata": {
        "id": "AYFO1RPJgrqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make('LunarLanderContinuous-v2')\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print(\"Action Space Size: \", env.action_space.shape)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "P2toRmfkmcl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSe5KOy9gFIN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#env = gymnasium.make(\"LunarLander-v2\", continuous=True)\n",
        "env = make_vec_env(\"LunarLanderContinuous-v2\", n_envs=1)\n",
        "\n",
        "#env_val = gymnasium.make(\"LunarLander-v2\", render_mode=\"rgb_array\", continuous=True)\n",
        "env_val = make_vec_env(\"LunarLanderContinuous-v2\", n_envs=1)\n",
        "\n",
        "eval_callback = EvalCallback(env_val,\n",
        "                             best_model_save_path=\"./logs/\",\n",
        "                             log_path=\"./logs/\",\n",
        "                             eval_freq=2000,\n",
        "                             render=False,\n",
        "                             n_eval_episodes=20)\n",
        "\n",
        "# Initialize PPO\n",
        "model = PPO('MlpPolicy', env, verbose=0, ent_coef=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500000,  progress_bar=True, callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_lunar_lander_continous\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#env = gymnasium.make(\"LunarLander-v2\", continuous=True)\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "\n",
        "#env_val = gymnasium.make(\"LunarLander-v2\", render_mode=\"rgb_array\", continuous=True)\n",
        "env_val = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "\n",
        "eval_callback = EvalCallback(env_val,\n",
        "                             best_model_save_path=\"./logs/\",\n",
        "                             log_path=\"./logs/\",\n",
        "                             eval_freq=2000,\n",
        "                             render=False,\n",
        "                             deterministic=True,\n",
        "                             n_eval_episodes=20)\n",
        "\n",
        "# Initialize PPO\n",
        "model = PPO('MlpPolicy', env, verbose=0, ent_coef=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500000,  progress_bar=True, callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_lunar_lander\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ],
      "metadata": {
        "id": "9bg639srE5fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "env = make_vec_env(\"LunarLanderContinuous-v2\", n_envs=1, seed=0)\n",
        "best_model_path = \"./logs/best_model.zip\"\n",
        "best_model = PPO.load(best_model_path, env=env)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(best_model, env, n_eval_episodes=20)\n",
        "print(f\"Best Model - Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Record video of the best model playing CarRacing\n",
        "env = VecVideoRecorder(env, \"./videos/\", video_length=5000, record_video_trigger=lambda x: x == 0, name_prefix=\"best_model_lunar_lander_ppo\")\n",
        "\n",
        "obs = env.reset()\n",
        "for _ in range(5000):\n",
        "    action, _states = best_model.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()\n",
        "    if dones:\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "OLhIFgNqoFEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the evaluations.npz file\n",
        "data = numpy.load('./logs/evaluations.npz')\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps, mean_results - std_results, mean_results + std_results, alpha=0.3)\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title('PPO Performance on LunarLander-v2')\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "id": "oUvrwfYXrWFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}